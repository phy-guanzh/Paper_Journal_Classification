@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@ARTICLE{708428,
  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal={IEEE Intelligent Systems and their Applications}, 
  title={Support vector machines}, 
  year={1998},
  volume={13},
  number={4},
  pages={18-28},
  keywords={Support vector machines;Machine learning;Algorithm design and analysis;Pattern recognition;Neural networks;Training data;Polynomials;Kernel;Character recognition;Web pages},
  doi={10.1109/5254.708428}}

@inbook{doi:https://doi.org/10.1002/9781119450382.ch10,

publisher = {John Wiley, Sons, Ltd},
isbn = {9781119450382},
title = {Multinomial Logistic Regression},
booktitle = {Categorical Data Analysis by Example},
chapter = {10},
pages = {109-124},
doi = {https://doi.org/10.1002/9781119450382.ch10},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119450382.ch10},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119450382.ch10},
year = {2016},
keywords = {adjacent-category models, contingency table, cumulative logits, discrete choice models, explanatory variables, multinomial logistic regression, response variable},
abstract = {Abstract This chapter expresses that the response variable had just two categories, to the case where several categories are possible. The underlying distribution is therefore now multinomial. Models for this situation were described as discrete choice models by McFadden. Unless there are many category combinations, data involving only categorical variables can be easily summarized using a contingency table. To understand the effects (if any) of the explanatory variables on the response variable, it may be helpful to report the data as proportions for the categories of the response variable. The chapter presents an example of the dependence of political allegiance on gender and social class. Proportional Odds models may involve any number of explanatory variables, but, for simplicity, it limits to cases involving a single explanatory variable. Cumulative logits involved all J categories of the ordinal response variable. Adjacent-category models used just two categories.}
}

@Inbook{Mucherino2009,
author="Mucherino, Antonio
and Papajorgji, Petraq J.
and Pardalos, Panos M.",
title="k-Nearest Neighbor Classification",
bookTitle="Data Mining in Agriculture",
year="2009",
publisher="Springer New York",
address="New York, NY",
pages="83--106",
abstract="The k-nearest neighbor (k-NN) method is one of the data mining techniques considered to be among the top 10 techniques for data mining [237]. The k-NN method uses the well-known principle of Cicero pares cum paribus facillime congregantur (birds of a feather flock together or literally equals with equals easily associate). It tries to classify an unknown sample based on the known classification of its neighbors. Let us suppose that a set of samples with known classification is available, the so-called training set. Intuitively, each sample should be classified similarly to its surrounding samples. Therefore, if the classification of a sample is unknown, then it could be predicted by considering the classification of its nearest neighbor samples. Given an unknown sample and a training set, all the distances between the unknown sample and all the samples in the training set can be computed. The distance with the smallest value corresponds to the sample in the training set closest to the unknown sample. Therefore, the unknown sample may be classified based on the classification of this nearest neighbor.",
isbn="978-0-387-88615-2",
doi="10.1007/978-0-387-88615-2_4",
url="https://doi.org/10.1007/978-0-387-88615-2_4"
}
@Inbook{Jin2010,
author="Jin, Xin
and Han, Jiawei",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="K-Means Clustering",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="563--564",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_425",
url="https://doi.org/10.1007/978-0-387-30164-8_425"
}

@Inbook{ref1,
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="TF--IDF",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="986--987",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_832",
url="https://doi.org/10.1007/978-0-387-30164-8_832"
}

@article{10.1145/2733381,
author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Zimek, Arthur and Sander, J\"{o}rg},
title = {Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2733381},
doi = {10.1145/2733381},
abstract = {An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan’s classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of “outlierness” can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a “flat” (i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {5},
numpages = {51},
keywords = {unsupervised and semisupervised clustering, outlier detection, hierarchical and nonhierarchical clustering, global/local outliers, data visualization, Density-based clustering}
}


@Manual{cluster,
    title = {cluster: Cluster Analysis Basics and Extensions},
    author = {Martin Maechler and Peter Rousseeuw and Anja Struyf and
      Mia Hubert and Kurt Hornik},
    year = {2024},
    url = {https://CRAN.R-project.org/package=cluster},
    note = {R package version 2.1.8 --- For new features, see the
      'NEWS' and the 'Changelog' file in the package source)},
  }

@Article{Breiman2001,
author={Breiman, Leo},
title={Random Forests},
journal={Machine Learning},
year={2001},
month={Oct},
day={01},
volume={45},
number={1},
pages={5-32},
abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
issn={1573-0565},
doi={10.1023/A:1010933404324},
url={https://doi.org/10.1023/A:1010933404324}
}


@inproceedings{10.1145/2939672.2939785,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}



@article{doi:10.1080/14786440109462720,
author = { Karl   Pearson   F.R.S. },
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor & Francis},
doi = {10.1080/14786440109462720},
}

@Manual{lm,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }

@incollection{HAN201239,
title = {2 - Getting to Know Your Data},
editor = {Jiawei Han and Micheline Kamber and Jian Pei},
booktitle = {Data Mining (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
address = {Boston},
pages = {39-82},
year = {2012},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-0-12-381479-1},
doi = {https://doi.org/10.1016/B978-0-12-381479-1.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123814791000022},
author = {Jiawei Han and Micheline Kamber and Jian Pei},
abstract = {Publisher Summary
This chapter is about getting familiar with the data. Knowledge about the data is useful for data preprocessing, the first major task of the data mining process. The various attribute types are studied. These include nominal attributes, binary attributes, ordinal attributes, and numeric attributes. Basic statistical descriptions can be used to learn more about each attribute's values. Given a temperature attribute, one can determine its mean (average value), median (middle value), and mode (most common value). These are measures of central tendency, which give us an idea of the “middle” or center of distribution. Knowing such basic statistics regarding each attribute makes it easier to fill in missing values, smooth noisy values, and spot outliers during data preprocessing. Knowledge of the attributes and attribute values can also help in fixing inconsistencies incurred during data integration. Plotting the measures of central tendency shows us if the data are symmetric or skewed. Quantile plots, histograms, and scatter plots are other graphic displays of basic statistical descriptions. These can all be useful during data preprocessing and can provide insight into areas for mining. The field of data visualization provides many additional techniques for viewing data through graphical means. These can help identify relations, trends, and biases “hidden” in unstructured data sets. The similarity/dissimilarity between objects may also be used to detect outliers in the data, or to perform nearest-neighbor classification. There are many measures for assessing similarity and dissimilarity. In general, such measures are referred to as proximity measures.}
}

@Article{stm,
    title = {{stm}: An {R} Package for Structural Topic Models},
    author = {Margaret E. Roberts and Brandon M. Stewart and Dustin
      Tingley},
    journal = {Journal of Statistical Software},
    year = {2019},
    volume = {91},
    number = {2},
    pages = {1--40},
    doi = {10.18637/jss.v091.i02},
  }

@ARTICLE{10.4108/eai.13-7-2018.159623,
    author={Pooja  Kherwa and Poonam Bansal},
    title={Topic Modeling: A Comprehensive Review},
    journal={EAI Endorsed Transactions on Scalable Information Systems},
    volume={7},
    number={24},
    publisher={EAI},
    journal_a={SIS},
    year={2019},
    month={7},
    keywords={Topic Modeling, Latent Dirichlet Allocation, Latent Semantic Analysis, Inference, Dimension reduction},
    doi={10.4108/eai.13-7-2018.159623}
}

@INPROCEEDINGS{8950616,
  author={Qader, Wisam A. and Ameen, Musa M. and Ahmed, Bilal I.},
  booktitle={2019 International Engineering Conference (IEC)}, 
  title={An Overview of Bag of Words;Importance, Implementation, Applications, and Challenges}, 
  year={2019},
  volume={},
  number={},
  pages={200-204},
  keywords={Bag of Words;Image Classification;Text Classification;Visual Scene Classification},
  doi={10.1109/IEC47844.2019.8950616}}

@article{10.1002/asi.24810,
author = {Kousha, Kayvan and Thelwall, Mike},
title = {Factors associating with or predicting more cited or higher quality journal articles: An Annual Review of Information Science and Technology (ARIST) paper},
journal = {Journal of the Association for Information Science and Technology},
volume = {75},
number = {3},
pages = {215-244},
doi = {https://doi.org/10.1002/asi.24810},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24810},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24810},
abstract = {Abstract Identifying factors that associate with more cited or higher quality research may be useful to improve science or to support research evaluation. This article reviews evidence for the existence of such factors in article text and metadata. It also reviews studies attempting to estimate article quality or predict long-term citation counts using statistical regression or machine learning for journal articles or conference papers. Although the primary focus is on document-level evidence, the related task of estimating the average quality scores of entire departments from bibliometric information is also considered. The review lists a huge range of factors that associate with higher quality or more cited research in some contexts (fields, years, journals) but the strength and direction of association often depends on the set of papers examined, with little systematic pattern and rarely any cause-and-effect evidence. The strongest patterns found include the near universal usefulness of journal citation rates, author numbers, reference properties, and international collaboration in predicting (or associating with) higher citation counts, and the greater usefulness of citation-related information for predicting article quality in the medical, health and physical sciences than in engineering, social sciences, arts, and humanities.},
year = {2024}
}

@article{10.1162/qss_a_00327,
    author = {Hanson, Mark A. and Barreiro, Pablo Gómez and Crosetto, Paolo and Brockington, Dan},
    title = {The strain on scientific publishing},
    journal = {Quantitative Science Studies},
    volume = {5},
    number = {4},
    pages = {823-843},
    year = {2024},
    month = {11},
    abstract = {Scientists are increasingly overwhelmed by the volume of articles being published. The total number of articles indexed in Scopus and Web of Science has grown exponentially in recent years; in 2022 the article total was ∼47\% higher than in 2016, which has outpaced the limited growth—if any—in the number of practicing scientists. Thus, publication workload per scientist has increased dramatically. We define this problem as “the strain on scientific publishing.” To analyze this strain, we present five data-driven metrics showing publisher growth, processing times, and citation behaviors. We draw these data from web scrapes, and from publishers through their websites or upon request. Specific groups have disproportionately grown in their articles published per year, contributing to this strain. Some publishers enabled this growth by hosting “special issues” with reduced turnaround times. Given pressures on researchers to “publish or perish” to compete for funding, this strain was likely amplified by these offers to publish more articles. We also observed widespread year-over-year inflation of journal impact factors coinciding with this strain, which risks confusing quality signals. Such exponential growth cannot be sustained. The metrics we define here should enable this evolving conversation to reach actionable solutions to address the strain on scientific publishing.},
    issn = {2641-3337},
    doi = {10.1162/qss_a_00327},
    url = {https://doi.org/10.1162/qss\_a\_00327},
    eprint = {https://direct.mit.edu/qss/article-pdf/5/4/823/2478590/qss\_a\_00327.pdf},
}